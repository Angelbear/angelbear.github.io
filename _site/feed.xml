<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-11-21T16:52:15+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Yangyang’ Blog</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><entry><title type="html">Tuning Istio</title><link href="http://localhost:4000/istio/2023/11/20/Tuning-Istio.html" rel="alternate" type="text/html" title="Tuning Istio" /><published>2023-11-20T19:00:00+08:00</published><updated>2023-11-20T19:00:00+08:00</updated><id>http://localhost:4000/istio/2023/11/20/Tuning-Istio</id><content type="html" xml:base="http://localhost:4000/istio/2023/11/20/Tuning-Istio.html">&lt;p&gt;Some of the new users of service mesh solutions including myself may think that network connections would be &lt;strong&gt;improved&lt;/strong&gt; by just introducing the solution. This is wrong!&lt;/p&gt;

&lt;p&gt;In fact, ever since introducing Istio, we’ve met more way networking issues with workloads enabled service mesh, compared to the workloads that doesn’t. There are many of the cases you might be supprised that you need to take care of once you start to use Istio.&lt;/p&gt;

&lt;p&gt;In this blog post, we will see how Istio makes things more complicated, and how we could tune the Istio setting to improve the networking connections.&lt;/p&gt;

&lt;h2 id=&quot;istio-proxy&quot;&gt;istio-proxy&lt;/h2&gt;

&lt;p&gt;istio-proxy, is like the wheels of the Pod, it hijacks all the network connections from/to the Pod after initialization.&lt;/p&gt;

&lt;p&gt;Before you adapt Istio, your client Pod and server Pod would be connected via a single TCP connection, when you experienced some networking issues, you would either find some evidence from client side or server side. You only have 2 places to look at.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/imgs/04-direct-connection.svg&quot; alt=&quot;Istio Proxy&quot; style=&quot;display: block; width: 70%; margin: auto; background: white;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After you adapt Istio, both your client Pod and server Pod will be injected with an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; container, and this container hijacks all TCP connections in / out of the Pod.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/imgs/04-istio-proxy.svg&quot; alt=&quot;Istio Proxy&quot; style=&quot;display: block; width: 70%; margin: auto; background: white;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So instead of 1 TCP connection, you will have to establish 3 TCP connections to “simulate” the original 1 TCP connection:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Between client Pod’ main container and its istio-proxy container&lt;/li&gt;
  &lt;li&gt;Between client Pod’s istio-proxy container and server Pod’s istio-proxy container&lt;/li&gt;
  &lt;li&gt;Between server Pod’s istio-proxy container and server Pod’s main container&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Supppose 1 TCP connection have 99.999% SLA, with istio enabled, you will have your 5 9’s SLA degraded to 4 9s. (99.999%&lt;sup&gt;3&lt;/sup&gt; = 99.997%) natually. And what’s more, there are pitfalls around its life cycles.&lt;/p&gt;

&lt;h3 id=&quot;startup-network-availability&quot;&gt;Startup network availability&lt;/h3&gt;

&lt;p&gt;One of the problems when met in early days with istio is the startup networking issue - in client Pod’s main container, our application launches and tries to access the network immediately.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/imgs/04-istio-proxy-start.svg&quot; alt=&quot;Istio Proxy&quot; style=&quot;display: block; width: 70%; margin: auto; background: white;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;While &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; works as a “sidecar” container, it is launched with the main container together. As a matter of fact, there is no “main” or “sidecar” concept in Kubernetes (there is a &lt;a href=&quot;https://github.com/kubernetes/enhancements/issues/753&quot;&gt;proposal&lt;/a&gt; in k8s but not procceeded yet.), both of them are containers. But if &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; container is not ready, all the other containers in the Pod will not be able to access network. And if you main container happen to start network access earlier than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; container ready, your main container may fail to start.&lt;/p&gt;

&lt;p&gt;Most of the ppl didn’t realie this start up gap, the main container’s process is not able to access network in the begining, and it may retry and eventually succeeded after istio-proxy is ready; Or they exit with non-zero command and the main container get restarted, evetually it will also succeed after istio-proxy is ready.&lt;/p&gt;

&lt;p&gt;Sometimes your application start is expensive and may have side effect, so you do not want the main container to be restarted. To mitigate this issue, we’ve added wrapper script to the main container to monitor the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; container’s readiness (exposed in port 15000) before it really start the final command.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Wait for istio sidecar starts&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;waited&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0
&lt;span class=&quot;k&quot;&gt;while &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do
   &lt;/span&gt;curl &lt;span class=&quot;nt&quot;&gt;--max-time&lt;/span&gt; 1 &lt;span class=&quot;nt&quot;&gt;--head&lt;/span&gt; localhost:15000 &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; /dev/null &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;break
   &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;waited&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;$((&lt;/span&gt;waited+1&lt;span class=&quot;k&quot;&gt;))&lt;/span&gt;
   &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Wait 1s for istio sidecar&quot;&lt;/span&gt;
   &lt;span class=&quot;nb&quot;&gt;sleep &lt;/span&gt;1
&lt;span class=&quot;k&quot;&gt;done
&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Istio sidecar is ready after &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;waited&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; seconds wait&quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Start real process&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The Istio team is aware of this issue, therefore from Istio 1.8+, a proxy config can be enabled to hold application container to start until &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; is ready.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;install.istio.io/v1alpha2&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;IstioOperator&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;meshConfig&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;defaultConfig&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;holdApplicationUntilProxyStarts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Some prodution practise shows that even if &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;holdApplicationUntilProxyStarts&lt;/code&gt; was introduced, the very first short time of the main container might still suffer from network issues, so if your app is very sensitive to network access in the startup phase, you may need to take care of the start process.&lt;/p&gt;

&lt;h3 id=&quot;graceful-termination&quot;&gt;Graceful Termination&lt;/h3&gt;

&lt;p&gt;Since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt;  plays as a “proxy”, in order to make sure your main container have 100% of network connectivity, you need to make sure &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt;’s availability is longer than your main container’s network availibility. That is, before your container start network access, istio-proxy needs to be ready, and it must be ready after your container drops the network.&lt;/p&gt;

&lt;p&gt;Networking issues also happens during the termination of the Pod. When a Pod is to be terminated, Kubernetes send &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SIGTERM&lt;/code&gt; to each of the Pod’s container, including your main container, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; container.&lt;/p&gt;

&lt;p&gt;As a kubernetes user, you might already be familiar with the &lt;a href=&quot;https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination&quot;&gt;termination in Pod lifecycle&lt;/a&gt;, so your application already handles &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SIGTERM&lt;/code&gt; in a graceful way - it first blocks any new connetions, and then finishes the existing connections’ requests, finally shutdown itself. At the same time we also configure &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;spec.terminationGracePeriodSeconds&lt;/code&gt; to give the main container’s process time for processing the exisiting requests.&lt;/p&gt;

&lt;p&gt;Howeve, the problem is that by default, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; container will “immediately” falls in to the shutdown process, and draining all the connactions it currently holds. So all of a sudden, your main container loses all network access, either inbound or outbound. This makes your Pod’s graceful shutdown not working anymore.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/imgs/04-istio-proxy-terminate.svg&quot; alt=&quot;Istio Proxy&quot; style=&quot;display: block; width: 70%; margin: auto; background: white;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’ve experienced this problem that when HorizontalPodAutoscaler scales down the server Deployment, there will be a lot of 503 client errors complaining about connetion failure.&lt;/p&gt;

&lt;p&gt;In order to solve this problem, Istio provides configuration hold the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; for given duration before draining all the connections, similar to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;holdApplicationUntilProxyStarts&lt;/code&gt;, it can be configured in the global isito settings.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;install.istio.io/v1alpha2&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;IstioOperator&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;meshConfig&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;defaultConfig&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;terminationDrainDuration&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;30s&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Although, I recommend you to adapt this setting per deployment because every deployment may have different patterns of graceful and different time duration needed.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;annotations&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;proxy.istio.io/config&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;|&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;terminationDrainDuration: 30s&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above annotation can be added in Pod spec so that it overrides’ the global settings, usually this should match the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;spec.terminationGracePeriodSeconds&lt;/code&gt; in the Pod spec.&lt;/p&gt;

&lt;p&gt;Updates: From Istio 1.12 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EXIT_ON_ZERO_ACTIVE_CONNECTIONS&lt;/code&gt; flag was introduced to terminates proxy when number of active connections become zero during draining.&lt;/p&gt;

&lt;h2 id=&quot;service-discovery&quot;&gt;Service Discovery&lt;/h2&gt;

&lt;p&gt;In the above section, we’ve talked about the connectivity issues between client / server Pod main container and their &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; container.&lt;/p&gt;

&lt;p&gt;What if the connnection issues happen between the source and destination Pod’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt;?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/imgs/04-istio-proxy-xDS.svg&quot; alt=&quot;Istio Proxy&quot; style=&quot;display: block; width: 70%; margin: auto; background: white;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Inter-pod communication is always complicated, either with or without Istio. The issue may happen in any layer of the network - kube-proxy, network cni, node issues, sometimes even physical issues in cloud providers.&lt;/p&gt;

&lt;p&gt;Since istio introduces extra ability to manage the connections for better traffic management, as the price, it has more possibilities of getting potential connectivity issues.&lt;/p&gt;

&lt;p&gt;A lot of times, application teams come and complain about connectivity issues. Basically there was intermediate connection failures between client and server deployments.&lt;/p&gt;

&lt;p&gt;A very common connectivity issues we’ve met since adapting Istio is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;503 upstream_reset_before_response_started&lt;/code&gt;, it’s printed from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; log in the client Pod when the pod tries to access the service pod via kubernetes service address.  The problem is happening from time to time, it’s not so critical but very annoying.&lt;/p&gt;

&lt;p&gt;Sometimes the issues became more frequent, and teams would ask SRE to look at the root cause. The dialogs have a very similar pattern:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(Client Team): Our client applications shows 100+ 503s within last hour, there were spikes of timeout events from time to time.
(Server Team): We&apos;ve checked our server application&apos;s logs, we have 99.999% successful rate of service calls, and for 100% of the requests, we always return results (either good or error) within 10 seconds.
(Client Team): We didn&apos;t touch any client code in last 2 weeks, so it&apos;s unlikely a client issue.
(Server Team): This backend is not changed over 3 months, so it&apos;s unlikely a server issues as well.
(SRE Team): We didn&apos;t touch any Istio settings recently, maybe the traffic was too much during the spike, let&apos;s increase the Pod number of the server pods.
... (SRE Team increases the server Pods number) ...
... Some time later ...
(Client Team): The 503s was a bit better when we scaled up, but after a while, 503 comes again.
(Server Team): We still confirmed 99.999% SLAs.
(SRE Team): ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To figure out the issue, we need to closely look into the istio-proxy access logs.&lt;/p&gt;

&lt;p&gt;The following is a failed service call log.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[2023-11-12T22:28:36.878Z] &quot;POST /service HTTP/1.1&quot; 503 UC 0 95 44456 - - &quot;20f7f7e8-49f1-448c-a007-8806deec0414&quot; &quot;yourdomain.com&quot; &quot;10.10.10.10:80&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From the log we could figure out some detailed information around this call - when did it happen, http status, byte transferred, conneciton time, response time, request id, request domain, upstream hosts, etc.&lt;/p&gt;

&lt;p&gt;The key field in the error log is the upstream host &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10.10.10.10:80&lt;/code&gt;, which is the exact IP / Port this request goes to. And this IP is actually the service Pod’s IP address. We can use this IP address to back search the actual service Pod name, and we may be able to find out the Pod’s logs to troubleshoot.&lt;/p&gt;

&lt;p&gt;We tried to back search the original Pod’s information of those 503 request logs, and we found those IPs are mostly from already terminated Pods.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/imgs/04-phantom-pod.svg&quot; alt=&quot;Istio Proxy&quot; style=&quot;display: block; width: 70%; margin: auto; background: white;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The connections made to the terminated Pods will never be successfully established, but instead of immediatly fail the connection, it will hung there until client Pod’s istio-proxy exceed the connection timeout limit.&lt;/p&gt;

&lt;p&gt;Why would this phantom connection happen? Why client Pod would connect to a Pod that is already terminated? Why can’t the client Pod’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; to try testing the pod liveness before making the connection?&lt;/p&gt;

&lt;h3 id=&quot;xds&quot;&gt;xDS&lt;/h3&gt;

&lt;p&gt;Let’s pick some memories about how load balancing was done with / without Istio in &lt;a href=&quot;./03-L4-Vs-L7.md&quot;&gt;L4 Vs L7&lt;/a&gt;. We can reuse the example of nginx, how does a client’s Pod’s istio-proxy choose a Pod to connect to when the client’s main container needs to create a TCP connection to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx-service.namespace.svc.cluster.local&lt;/code&gt;?&lt;/p&gt;

&lt;p&gt;The answer is - service Pods need to register / deregister themselves, and client pods need to update the service Pods’ registeration information.&lt;/p&gt;

&lt;p&gt;The design philosophy of istio is that it keeps most of the information needed by runtime in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt;, and the information is got from centralized control plane - istiod.&lt;/p&gt;

&lt;p&gt;So you can understand like this: In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt;’s memory, it contains a map that holds information like following. Basically we need to know which service have what Pods, and each Pod’s status. (healthy, unhealthy, etc)&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;serviceA  ------ serviceA-pod-1 (healthy)
            |___ serviceA-pod-2 (unhealthy)
            |___ serviceA-pod-3 (healthy)
            ...

serviceB ------- serviceB-pod-1 (healthy)
            |___ serviceB-pod-2 (healthy)
            |___ serviceB-pod-3 (unhealthy)
            ...            
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When a Pod is created and injected with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt;, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; container gets this full service map from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istiod&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Of course, service Pods also changes on the fly. So when a service Pod is created / terminated / changed probe status, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istiod&lt;/code&gt; will receive the information from either the service Pod’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt;, or it proactively monitor the service events from kubernetes, it will update the service map entries, and then eventually it will push these updates to all the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; containers on the kubernetes.&lt;/p&gt;

&lt;p&gt;The protocol of these service entry updates is called &lt;strong&gt;xDS&lt;/strong&gt;. You might have seen tthishe word in Istio’s documentation once or twice, it appears more often in envoy’s documentation. (For relationship between Istio and Evnoy, please see the previous &lt;a href=&quot;./02-Istio-Vs-Envoy.md&quot;&gt;article&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;The word &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xDS&lt;/code&gt; contains two parts - &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x&lt;/code&gt; is one of the &lt;strong&gt;C&lt;/strong&gt;luster / &lt;strong&gt;L&lt;/strong&gt;istener / &lt;strong&gt;E&lt;/strong&gt;ndpoint / &lt;strong&gt;R&lt;/strong&gt;oute’s initial and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DS&lt;/code&gt; is short for Discovery Service. The process of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istiod&lt;/code&gt; synchronizing these information to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; is called xDS push.&lt;/p&gt;

&lt;p&gt;xDS push doesn’t come for free, it takes time, especially if there are a lot of services in the kubernetes cluster, or a lot of Pods injected with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; container, or there are very frequent service updates happening.&lt;/p&gt;

&lt;p&gt;Say a server Pod is terminated, it takes &lt;em&gt;T&lt;/em&gt; time before all client Pods’ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; updated their own service map to remove this Pod from the list. But during the &lt;em&gt;T&lt;/em&gt; time, there is still possiblity the server Pod is picked by the client Pods for establishing TCP connection. Once it happend, it caused the issue we met above.&lt;/p&gt;

&lt;p&gt;Once we’ve reconized this issue, we could think of multiple ways to mitigate it.&lt;/p&gt;

&lt;h4 id=&quot;reduce-xds-push-time&quot;&gt;Reduce xDS push time&lt;/h4&gt;

&lt;p&gt;The very direct solution to mitigate the phantom connection is to reduce the &lt;em&gt;T&lt;/em&gt; time. Istio exposes metrics for you to observe xDS push performance, and usually &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pilot_proxy_convergence_time&lt;/code&gt; is the key metric you should monitor.&lt;/p&gt;

&lt;p&gt;There are many different ways to reduce the push time.&lt;/p&gt;

&lt;h4 id=&quot;increase-istiod-replicas&quot;&gt;Increase istiod replicas&lt;/h4&gt;
&lt;p&gt;Since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istiod&lt;/code&gt; plays the key role of collecting and pushing xDS messages from / to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt;, more replicas would help to reduce the ops throughput of 1 istiod Pod, you should spend some time to adjust the istiod’s kubernetes Pod spec and HorizontalPodAutoscaling and make sure there is sufficient resource allocated to it.&lt;/p&gt;

&lt;p&gt;Following is the example of tuning istiod in IstioOperator.&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;install.istio.io/v1alpha1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;IstioOperator&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;components&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;pilot&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;k8s&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;300m&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;memory&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;2Gi&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;hpaSpec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;minReplicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;8&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;maxReplicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;64&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Resource&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;resource&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cpu&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;targetAverageUtilization&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;remove-unnessary-xds-push&quot;&gt;Remove unnessary xDS push&lt;/h4&gt;

&lt;p&gt;We could increase istiod number to reduce the workload of each istiod, but the overal xDS push volume doesn’t reduce. If we can reduce the xDS push volume, that would be a more effective way to improve xDS push delay.&lt;/p&gt;

&lt;p&gt;In the above section we’ve described how client’s istio-proxy would memorize the service map with the xDS push process. With &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istioctl&lt;/code&gt; you would be able to inspect the mapping on the fly.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;~ istioctl proxy-config cluster client-pod-1.namespace
SERVICE FQDN                                          PORT     SUBSET                               DIRECTION     TYPE             DESTINATION RULE
                                                      80       -                                    inbound       ORIGINAL_DST
BlackHoleCluster                                      -        -                                    -             STATIC
InboundPassthroughClusterIpv4                         -        -                                    -             ORIGINAL_DST
InboundPassthroughClusterIpv6                         -        -                                    -             ORIGINAL_DST
PassthroughCluster                                    -        -                                    -             ORIGINAL_DST
agent                                                 -        -                                    -             STATIC
server.namespace.svc.cluster.local                    80       v1                                   outbound      EDS              
server.namespace.svc.cluster.local                    80       v2                                   outbound      EDS      
otherservice1.namespace.svc.cluster.local             80       -                                    outbound      EDS              
otherservice2.namespace.svc.cluster.local             80       -                                    outbound      EDS   
...
prometheus_stats                                      -        -                                    -             STATIC
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above commane &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;proxy-config cluster&lt;/code&gt; shows the CDS configuration, you could see that it contains some common services like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BlackHoleCluster&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;InboundPassthroughClusterIpv(4|6)&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PassthroughCluster&lt;/code&gt;, etc., it also contains the kubernetes service clusters, such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;server.namespace.svc.cluster.local&lt;/code&gt; and others.&lt;/p&gt;

&lt;p&gt;If you inspect this in a real kubernetes cluster without proper tuning of Isito, you will find one of the biggest pitfall of Istio’s default setting - It will make &lt;strong&gt;every single Pod&lt;/strong&gt; to receive the xDS push for &lt;strong&gt;every single kubernetes service&lt;/strong&gt;!&lt;/p&gt;

&lt;p&gt;Ideally in above example, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;client-pod-1&lt;/code&gt; only need to receive xDS push for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;server.namespace.svc.cluster.local&lt;/code&gt;, but it ended up receiving xDS push for all other services in the kubernetes cluster, so it is with all the other Pods injected with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; in the kuberentes cluster. So the xDS push volume grows bigger and bigger with more Pods and service count.&lt;/p&gt;

&lt;p&gt;What is the ideal situation is that every Pod only receives nessasary xDS push for the services that 1) it needs to connect to the service, 2) it needs to make use of the L7 proxy features.&lt;/p&gt;

&lt;p&gt;But if you just follow Istio official examples, they won’t tell you to tune this since xDS push won’t be a bottleneck for simple senarios. But it is definitely worth tuning if you have more than dozens of services and more than thounsands of Pods on the same kubernetes clusters.&lt;/p&gt;

&lt;p&gt;The key Istio component to tune this is &lt;a href=&quot;https://istio.io/latest/docs/reference/config/networking/sidecar/&quot;&gt;Sidecar&lt;/a&gt;, this “Sidecar” is not the “sidarcar” container which is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt;, but rather to control “who cares what”.&lt;/p&gt;

&lt;p&gt;For the very first step, I would highly recommend you to add a default &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Sidecar&lt;/code&gt; for every namespace you would like to enable istio injection.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;networking.istio.io/v1beta1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Sidecar&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;default&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;egress&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;hosts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;~/*&apos;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;outboundTrafficPolicy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ALLOW_ANY&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;What does this do?&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;default&lt;/code&gt; Sidecar without &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;workloadSelector&lt;/code&gt; will be the fallback rule for all &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; containers in this namespace Pods.&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;egress.hosts[0]&lt;/code&gt; with value &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/*&lt;/code&gt; means by default &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; container will not receive any service xDS push from any namespace&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;outboundTrafficPolicy.mode&lt;/code&gt; with value &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ALLOW_ANY&lt;/code&gt; means &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; will delegate the traffic control back to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kube-proxy&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Simply by setting this up, we will reduce 99% of the xDS pushes, but it also means we loses the the L7 layer traffic management features provided by Istio, and the behavior rolled back to kubernetes default.&lt;/p&gt;

&lt;p&gt;We definitely still want the L7 layer traffic management between the client and server, so we could add additional Sidecar configuration to cover this case.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;networking.istio.io/v1beta1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Sidecar&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;client-sdc&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;egress&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;hosts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;namespace/server.namespace.svc.cluster.local&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;workloadSelector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;client&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above configuration allows the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;client&lt;/code&gt; Pods to be registered with xDS push for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;server.namespace.svc.cluster.local&lt;/code&gt; service changes, and the traffic management is taken over by Istio.&lt;/p&gt;

&lt;p&gt;With the two types of Sidecar above, we’ve changed the “all in xDS” behavior to “selective xDS”, and from my testing on a real prodution kubernetes cluster, it reduces 80% of the xDS push and dramatically improved the istiod resource consumption and the xDS push time.&lt;/p&gt;

&lt;h4 id=&quot;outlier-detection&quot;&gt;Outlier detection&lt;/h4&gt;

&lt;p&gt;We can do a lot of efforts to reduce the xDS push time, however you could never reduce it to 0. After the optimization we’ve done in previous sections, we’ve reduced the push time to &amp;lt; 100ms in p99. This is already great improvement, but 100ms is still relatively long compared to the QPS that our cliend Pod requesting server Pod, so whenever a server Pod is terminated, there are still 503 errors from client to server.&lt;/p&gt;

&lt;p&gt;When we take a look at the logs and try to find out all the failed requests during a period, we found very interesting patterns - The failed requests were distributed among different client Pods, and each Pod failed for no more than 5 times. It seems that the client Pod will passively retry for other hosts if it experience a certain number of connection failures to a specific server Pod.&lt;/p&gt;

&lt;p&gt;This is yet another advanced traffic management feature that provided by Istio, which is not covered in &lt;a href=&quot;./03-L4-Vs-L7.md&quot;&gt;L4 Vs L7&lt;/a&gt;, called &lt;strong&gt;Outlier Detection&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;xDS push will make the server Pods status to be eventually synced to the client Pods, and outlier detection is an addition support - before xDS push arrive, if the client Pod already experienced X times connection error to the destination Pod, it will mark it an “outlier” locally and avoid using it for a while.&lt;/p&gt;

&lt;p&gt;Outlier detection is by default enabled, and default values can be found in the &lt;a href=&quot;https://istio.io/latest/docs/reference/config/networking/destination-rule/#OutlierDetection&quot;&gt;official documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;One “pitfall” of the default outlier detection is that, it will only mark a upstream Pod to be outliered after 5 consecutive 5xx errors, that’s matching what we’ve observed from the client logs. The problem is that under this case, we’d want the client Pod to immediately choose another Pod for the next fortune, instead of meeting 5 failures in a row.&lt;/p&gt;

&lt;p&gt;So we’ve made following adjustment to let this outlier detection to be more effective.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;networking.istio.io/v1alpha3&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;DestinationRule&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;server.namespace.svc.cluster.local&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;trafficPolicy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;outlierDetection&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;baseEjectionTime&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;10s&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;consecutive5xxErrors&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;consecutive5xxErrors&lt;/code&gt; is configured to be 1, which is most aggressive to make the client Pod choose another Pod for service connection after 1 503 timeout events.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;baseEjectionTime&lt;/code&gt; is the time that outlier status will initially last before client Pod retries the Pod again, it is configured to be 10 seconds, and most likely within  this time, xDS push will arrive and remove this Pod from xDS entries entirely.&lt;/p&gt;

&lt;p&gt;With the above setting, we’ve seen a client Pod will at most experiencing 503 once for a server Pod, and the timeout events reduced to 20% ~ 30% percent compared to before.&lt;/p&gt;

&lt;p&gt;The outlier detection has some very detailed and interesting settings, and I suggest you taking a look at the documenetation and fine tune it for your own use case.&lt;/p&gt;

&lt;h4 id=&quot;retry-retry-and-retry&quot;&gt;Retry, retry and retry&lt;/h4&gt;

&lt;p&gt;After tuning wthe outlier detection, we are actually pretty satisfied with the sucessful rate, but the 503s still exists, if we have 100 client pods, it might cause 100 timeouts when a server Pod is terminated.&lt;/p&gt;

&lt;p&gt;Could we aim for zero 503s?&lt;/p&gt;

&lt;p&gt;The answer is Yes.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VirtualService&lt;/code&gt; supports configuration of retries for HTTP routes. It allows you to configure how many times you want to retry, and under what conditions you want to retry, etc.&lt;/p&gt;

&lt;p&gt;Originally we’ve configured retries as following, to let it retry at most 3 times when encounting gateway error, connection failures, etc.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Source: houzz.c2-thrift.flip/templates/vs.yaml&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;VirtualService&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-service-vs&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-service.namespace.svc.cluster.local&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;http&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;main&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;route&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;destination&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-service.namespace.svc.cluster.local&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;retries&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;attempts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;retryOn&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;gateway-error,connect-failure,refused-stream,reset,503&apos;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But from the results, it doesn’t work out well for the 503s to terminated Pods, and by carefully checking the documentation, we found this pitfall.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;http&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;retries&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
       &lt;span class=&quot;na&quot;&gt;attempts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;
       &lt;span class=&quot;na&quot;&gt;retryOn&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;gateway-error,connect-failure,refused-stream,reset,503&apos;&lt;/span&gt;
       &lt;span class=&quot;na&quot;&gt;retryRemoteLocalities&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http.retries.retryRemoteLocalities&lt;/code&gt; is a flag to configure whether the retry should be to another Pod, or the same Pod, and default value is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false&lt;/code&gt;. The perfectly explains why it doesn’t work - the destination Pod is already terminated, it won’t succeed no matter how many times you retries the same Pod. Switch this value to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt; makes the retry to retry another Pod, and in most cases it would work, even if it’s very unlucky to connect to another terminated Pod, it have 3 attempts so it most likely to get through eventually.&lt;/p&gt;

&lt;p&gt;With this final tuning, we’ve successfully made 503 timeout issues from several hundreds per day to nearly 0! This is definitely better than prior to use Istio.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this blog post, we could see how we identify and tune the Istio step by step, and finally achieved “better” connection than without using Istio.&lt;/p&gt;

&lt;p&gt;Waiting for dependency, graceful shutdown, outlier detection, retries, these are not new concepts created by Istio, developers used to implement these logics themselves in their application code. Istio make the logic embed in the service mesh layer and make it completely transparent to application.&lt;/p&gt;

&lt;p&gt;Also, it is noticable that the advanced traffic management features (outlier, retry) only supports L7 layer protocols (HTTP, gRPC, etc), if you want your system to benefit from it, you would choose your application protocols wisely.&lt;/p&gt;</content><author><name>Yangyang Zhao</name></author><category term="istio" /><summary type="html">Some of the new users of service mesh solutions including myself may think that network connections would be improved by just introducing the solution. This is wrong!</summary></entry><entry><title type="html">L4 Vs L7</title><link href="http://localhost:4000/istio/2023/11/20/L4-Vs-L7.html" rel="alternate" type="text/html" title="L4 Vs L7" /><published>2023-11-20T18:00:00+08:00</published><updated>2023-11-20T18:00:00+08:00</updated><id>http://localhost:4000/istio/2023/11/20/L4-Vs-L7</id><content type="html" xml:base="http://localhost:4000/istio/2023/11/20/L4-Vs-L7.html">&lt;p&gt;As many of the students who have studied computer science, networking is definitely a part that is very important, but easy to forget after graduation.&lt;/p&gt;

&lt;p&gt;All the modern frameworks in different languages hide the details of networking from you, and you can use just 3 lines to make a HTTP call in Python.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;requests&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You take for granted that this is the ability provided, and may never remember those terminologies like physical layer, TCP handshake, etc. (Unless one day you were asked the question &lt;a href=&quot;https://www.linkedin.com/pulse/what-happens-when-you-type-googlecom-your-browser-press-sule-bala/&quot;&gt;What happens when you type google.com in your browser and press Enter&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;We are not going to cover that many details about networking in this article. But there is a key quesition that we should be able to answer, why do we introduce Istio for the first place?&lt;/p&gt;

&lt;p&gt;Stories may vary for different organization, but I guess the main purpose of introducing Istio should have &lt;strong&gt;better&lt;/strong&gt; traffic management in Kubernetes. The word “better” was emphasized because it’s not that kubernetes can not do traffic management. Kubernetes does provide traffic management, and in most senarios it works fine so that you do not need a better solution.&lt;/p&gt;

&lt;p&gt;What exactly is Traffic Management in Kubernetes? Simply speaking, to control how requests will be routed to Pods.&lt;/p&gt;

&lt;p&gt;Let’s take a look at a simple example. We first create a kubernetes deployment of nginx,&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apps/v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-deployment&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx:latest&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;and create corresponding service for accessing the nginx http server.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Service&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-service&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;TCP&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;targetPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ClusterIP&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Three &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx-deployment&lt;/code&gt; pods will be created, and you try to test the nginx service by typing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;curl http://nginx-service.namepace.svc.cluster.local&lt;/code&gt; in any pod on the same kubernetes cluster. It would output “It Works!” as the default nginx response.&lt;/p&gt;

&lt;h2 id=&quot;traffic-management---load-balancing&quot;&gt;Traffic Management - Load balancing&lt;/h2&gt;

&lt;p&gt;How did the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;curl&lt;/code&gt; commmand reached the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx-deployment&lt;/code&gt; pods? The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;curl&lt;/code&gt; command uses HTTP protocol, and it needs to establish a TCP connection to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx-service.namepace.svc.cluster.local&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;80&lt;/code&gt; port. Since there are three &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx&lt;/code&gt; Pods, how does kubernetes decide which Pod’s nginx to connect?&lt;/p&gt;

&lt;p&gt;Let’s see how it is working without Istio.  As the native kubernetes service implementation, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kube-proxy&lt;/code&gt; is responsible for the decision. Once &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx-service&lt;/code&gt; is created, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kube-proxy&lt;/code&gt; takes care of following things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;It creates IP address for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx-service&lt;/code&gt; and persist the record to kube-dns / core-dns, so when curl tries to resolve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx-service.namepace.svc.cluster.local&lt;/code&gt; it will get an dedicated Service IP.&lt;/li&gt;
  &lt;li&gt;Whenever a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx&lt;/code&gt; Pod is created / deleted, become ready / unready, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kube-proxy&lt;/code&gt; needs to add / remove the Pod from / to available candidates for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx-service&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Whenever a TCP connection is to be established to the service IP / port, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kube-proxy&lt;/code&gt; needs to select one candidate from its internal maintained list.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This is so-called “Load Balancing”. The load balancing strategy may varies in different kubernetes versions, in high level it will do sort of “round robin” to let each of the Pod to be able to receive connections with even possibility (at best effort).&lt;/p&gt;

&lt;h3 id=&quot;l4-layer-load-balancing---kube-proxy&quot;&gt;L4 layer load balancing - kube-proxy&lt;/h3&gt;

&lt;p&gt;The above process generally works well for most of the senarios. If you get 30 QPS requests to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx-service&lt;/code&gt;, theoritically it should have ~ 10 QPS requests to each of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx-deployment&lt;/code&gt; Pod.&lt;/p&gt;

&lt;p&gt;But in reality, you may find QPS is not evenly distributed in the 3 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx-deployment&lt;/code&gt; Pod, in some extreme cases, 1 Pod may get 30 QPS requests while other 2 Pods don’t get traffic at all.&lt;/p&gt;

&lt;p&gt;How does it happen?&lt;/p&gt;

&lt;p&gt;HTTP Protocol relies on TCP connections, so 1 HTTP request means we need to have 1 TCP connection. But it’s not like every HTTP request has to create a new TCP connection. There is this “Keep Alive” mechanism that a client can establish 1 TCP connection and reuse it for multiple HTTP requests.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/imgs/03-http-imbalanced.svg&quot; alt=&quot;HTTP Imbalanced Trffic&quot; style=&quot;display: block; width: 75%; margin: auto; background: white;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The above graph can directly simulate the actual traffic from different clients to different nginx pods. Although the TCP connections are evenly distributed, but actual HTTP requests are not.&lt;/p&gt;

&lt;p&gt;This is exactly the bottleneck of kubernete’s native traffic management - it only works at &lt;strong&gt;L4 (Transport) Layer&lt;/strong&gt;, while HTTP protocol is at &lt;strong&gt;L7 (Application) Layer&lt;/strong&gt;. Once TCP connections are established, kubernetes lose the control how the client will use the connection, such as protocol, frequency, etc.&lt;/p&gt;

&lt;h3 id=&quot;l7-layer-load-balancing---istio&quot;&gt;L7 layer load balancing - Istio&lt;/h3&gt;

&lt;p&gt;How things would be different if we adapt Istio?&lt;/p&gt;

&lt;p&gt;If we adapted istio and enabled the proxy injection in both service and client pods, we will see that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; container will be injected into the pods. Every single TCP connections from and to the pod will be intercepted.&lt;/p&gt;

&lt;p&gt;As described in &lt;a href=&quot;02-Istio-Vs-Envoy.md&quot;&gt;Istio VS Envoy&lt;/a&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;envoy&lt;/code&gt; does the the real job to establish the TCP connections to both side.&lt;/p&gt;

&lt;p&gt;Essentially &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;envoy&lt;/code&gt; needs to do the same job as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kube-proxy&lt;/code&gt;, but it does not rely on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kube-proxy&lt;/code&gt;, instead it implemented its ownd mechanism to do the service dns resolve, maintain the active candidates of Pods, and do the choice of pod for TCP connection.&lt;/p&gt;

&lt;p&gt;But &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;envoy&lt;/code&gt; does a bit more than that, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;envoy&lt;/code&gt; maintains the TCP connections to both sides, not not nessasarily a 1:1 mapping.&lt;/p&gt;

&lt;p&gt;Look at the following graph as the example, client app tries to connect to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx-service.namepace.svc.cluster.local&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;envoy&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; established the TCP connection between itself and the client process. But on the other hand, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;envoy&lt;/code&gt; would create (or reuse) 1 or more TCP connections between itself and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx-deployment&lt;/code&gt; Pods.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/imgs/03-http-balanced.svg&quot; alt=&quot;HTTP Imbalanced Trffic&quot; style=&quot;display: block; width: 70%; margin: auto; background: white;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Later on, when clint process tries to send GET http requests, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; would use some strategy to forward the requests to differnt TCP connections with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx-deployment&lt;/code&gt; Pods. As a result, even the client pods may request at different frequency, ideally we could still achieve average QPS load in each of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx-deployment&lt;/code&gt; Pods.&lt;/p&gt;

&lt;p&gt;This is so-called the &lt;strong&gt;L7 (Application) Layer&lt;/strong&gt; load balancing, and this is the ability we would want from the Istio service mesh solution.&lt;/p&gt;

&lt;p&gt;Not all of types the requests can benefit from the traffic management. For example, if you have a service app serves as TCP server and use custom Protocol, istio can not do that much to help with the load balancing. HTTP Protocol can be managed in this way because it’s a “stateless” protocol, so &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; can route the requests from the same client to different destination.&lt;/p&gt;

&lt;h3 id=&quot;load-balancing-tuning-with-istio&quot;&gt;Load balancing tuning with Istio&lt;/h3&gt;

&lt;p&gt;Once we realized that istio has the ability to manage traffic in L7 layer, we probably wanted to tune it to satisfy our application’s characteristics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://istio.io/latest/docs/reference/config/networking/destination-rule/&quot;&gt;DestinationRule&lt;/a&gt; is the key setting to tune traffic loadbalancing in L7 layer. It offers varities of configurations for users from simple strategies to advanced ones.&lt;/p&gt;

&lt;h4 id=&quot;simple-load-balancing&quot;&gt;Simple load balancing&lt;/h4&gt;
&lt;p&gt;Most of the users would be satisified &lt;a href=&quot;https://istio.io/latest/docs/reference/config/networking/destination-rule/#LoadBalancerSettings-SimpleLB&quot;&gt;standard load balancing algorithms&lt;/a&gt; provided by Istio, that require no tuning.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;networking.istio.io/v1alpha3&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;DestinationRule&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-service-dr&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-service.namespace.svc.cluster.local&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;trafficPolicy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;loadBalancer&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;simple&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ROUND_ROBIN&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LEAST_REQUEST&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ROUND_ROBIN&lt;/code&gt; would be the most practical algorithms that you might want to adapt to make the requests evenly distributed in your workloads. 99% of the time it is enough to do so.&lt;/p&gt;

&lt;h4 id=&quot;consistent-hash-load-balancing&quot;&gt;Consistent hash load balancing&lt;/h4&gt;

&lt;p&gt;Sometimes, you may want the traffic with the same characteristics to go to the same Pod for some reasons. For example, you may have in memory cache that may speed up the subsequential requests from the same user, or you may support resume from break-point for the same file uploads, etc.&lt;/p&gt;

&lt;p&gt;To satisfy the the requirement of traffic “stickness”, istio offers &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;consistentHash&lt;/code&gt; strategy to let you to configure to use some of the traffic’s information - header, cookie, ip, etc. as the source for calculating hash, any traffic hashed to the same bucket would go to the same destination pod. Following is a simple example to use a specific header for consistent hash.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;networking.istio.io/v1alpha3&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;DestinationRule&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-service-dr&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-service.namespace.svc.cluster.local&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;trafficPolicy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;loadBalancer&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;consistentHash&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;httpHeaderName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;my-header&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Notice that: 1. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;consistentHash&lt;/code&gt; is only best effort, it will be impacted by pods scale up / down. 2. After applying this strategy, you may experience traffic imbalance due to the nature of user traffic distribution. More settings can be referenced in &lt;a href=&quot;https://istio.io/latest/docs/reference/config/networking/destination-rule/#LoadBalancerSettings-ConsistentHashLB&quot;&gt;the official documentation&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;traffic-management---routing&quot;&gt;Traffic Management - Routing&lt;/h3&gt;

&lt;p&gt;After checking out the above settings, you might be pretty amazed by Istio already. But these are not enough to make Istio a good service mesh solution.&lt;/p&gt;

&lt;p&gt;One senario we leverage Istio for advanced traffic management is for release management.&lt;/p&gt;

&lt;p&gt;To extend then nginx example in previous sections, now suppose we already have a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx-deployment&lt;/code&gt; deployment that is using nginx:3.24, and we would like to upgrade it to use nginx:3.25. How would you approach it?&lt;/p&gt;

&lt;p&gt;The simpliest way is to just editing the original deployment’s pod spec, and switch the image from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx:3.24&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx:3.25&lt;/code&gt;, once you’ve done the operation, kuernetes will automatically trigger a rollout restart of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx-deployment&lt;/code&gt;, and old pods with 3.24 version will be terminated, and new pods with 3.25 version will be created.&lt;/p&gt;

&lt;p&gt;This works for some non-essential applications which you are either very confident to do a direct rollout or you are tolerant to temporary service down time so that you can easily rollback by switch the image version back.&lt;/p&gt;

&lt;p&gt;Sometimes your application is essential, and you definitely do want to do a phase rollout - you may want to test new version first with a small portion of traffic, and ramp up the traffic to new version higher, and eventually 100% to the new version if everything is good; Otherwise, you might want to ramp down the traffic to 0% to the new version, and 100% to the old version.&lt;/p&gt;

&lt;p&gt;It is possible to achieve this without service mesh solutions, but not perfectly. Let’s say we could always create multiple deployments of our applications with different versions.&lt;/p&gt;

&lt;p&gt;For example, we create a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx-deployment-v1&lt;/code&gt; deployment with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx:3.24&lt;/code&gt; image.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apps/v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-deployment-v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;   
      &lt;span class=&quot;na&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;1&lt;/span&gt;  
  &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx:3.24&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;And we create another deployment &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx-deployment-v2&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx:3.25&lt;/code&gt; image.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apps/v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-deployment-v2&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx:3.25&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The two deployments have the same label &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;app: nginx-service&lt;/code&gt;, so the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx-service&lt;/code&gt; Service will recogonize both deployment’s apps.&lt;/p&gt;

&lt;p&gt;We first will let &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx-deployment-v2&lt;/code&gt; to have 0 replicas, therefore TCP connections to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx-service.namepace.svc.cluster.local&lt;/code&gt; are all with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx-deployment-v1&lt;/code&gt; Pods. As we want to ramp up new version traffic, we could increase &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx-deployment-v2&lt;/code&gt; Pod replicas to 1, therefore there will be 3 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx-deployment-v1&lt;/code&gt; pods and 1 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx-deployment-v2&lt;/code&gt;, with the native kubernetes service load balancing, there is 25% possibility client Pods connecting to v2 pod and 75% possibility connecting to v1 pods. We could keep increaseing v2 pods replicas and reduce v2 pods replicas, until finally v1 pods scale down to 0, all traffic will go to v2 pods. If we find problems with v2 pods, we can revert the operation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/imgs//03-http-imbalanced-route.svg&quot; alt=&quot;HTTP Imbalanced Trffic&quot; style=&quot;display: block; width: 59%; margin: auto; background: white;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This process is just fine for normal release requrement, which you will have sort of the live / canary mechanism to test the new rollout. But this is not perfect because the granularity of the ramp up / down can not be precisely controlled.&lt;/p&gt;

&lt;p&gt;First of all, we have 3 replicas of old pods, so even ramping up 1 new pod will cause ~25% connections connecting to the new Pod.&lt;/p&gt;

&lt;p&gt;Secondly, due to the L4 layer load balancing limitation, even if we setup old / new pods to be 3 : 1, the actual HTTP request % may diverge due to some clients calling more / less frequently.&lt;/p&gt;

&lt;p&gt;How could Istio manage the routing better?&lt;/p&gt;

&lt;p&gt;Istio would allow we define different “subsets” of a service by adding name with label selectors in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DestinationRule&lt;/code&gt; - Remember we used this resource in above sections for load balancing? It can also be used for categorizing the service pods.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;networking.istio.io/v1alpha3&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;DestinationRule&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-service-dr&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-service.namespace.svc.cluster.local&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;subsets&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v2&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above config setup 2 subsets of nginx-service, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v1&lt;/code&gt; subset containing nginx:3.24, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v2&lt;/code&gt; subset containing nginx:3.25.&lt;/p&gt;

&lt;p&gt;After setting up DestinationRule, we setup a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VirtualService&lt;/code&gt; resource to define the route behavior.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;networking.istio.io/v1alpha3&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;VirtualService&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-service-vs&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-service.namespace.svc.cluster.local&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;http&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;route&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;destination&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-service&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;99&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;route&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;destination&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-service&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v2&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;We defined &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http.route&lt;/code&gt; config in VirtualService and let 99% of the http requests goes to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v1&lt;/code&gt; subset, and 1% of the traffic goes to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v2&lt;/code&gt; subset.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/imgs/03-http-balanced-route.svg&quot; alt=&quot;HTTP Imbalanced Trffic&quot; style=&quot;display: block; width: 59%; margin: auto; background: white;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The client app’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; will based on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VirtualService&lt;/code&gt; setting, to control the traffic split behavior. For example, if a client app sends 100 requests, 99 requests would go to v1 pods, 1 request would go to v2 pods. So even we have 3 v1 pods, and 1 v2 pods, we could still achieve 1% traffic split safely.&lt;/p&gt;

&lt;h3 id=&quot;advanced-traffic-route-rules&quot;&gt;Advanced traffic route rules&lt;/h3&gt;

&lt;p&gt;We see how the combination of VirtualService and DestinationRule controls the traffic split to v1 / v2 nginx nicely, but this is not the only ability of it.&lt;/p&gt;

&lt;p&gt;Sometimes you want to test certain endpoints with specfic subsets - for example you want a certain URL pattern goes to some pods.&lt;/p&gt;

&lt;p&gt;It’s very straightforward in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VirtualService&lt;/code&gt; config that you can add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;match&lt;/code&gt; rules in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http.route&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;networking.istio.io/v1alpha3&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;VirtualService&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-service-vs&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-service.namespace.svc.cluster.local&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;http&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;prefix&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/v2&quot;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;route&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;destination&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-service&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v2&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;route&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;destination&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-service&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above config would make it possible that:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;curl http://nginx-service.namespace.svc.cluster.local/v2&lt;/code&gt; goes to v2 pods&lt;/li&gt;
  &lt;li&gt;Other requests goes to v1 pods&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Another common senario is that, you have a public facing web service, and before rolling out the new version, you want test the new versions on your browser using same urls, but “normal” users still access the old version.&lt;/p&gt;

&lt;p&gt;A simple way to let Istio to identify “test” traffic would be adding a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;debug=&lt;/code&gt; cookie, and the requests with the cookie set would go to the new version.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;networking.istio.io/v1alpha3&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;VirtualService&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-service-vs&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-service.namespace.svc.cluster.local&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;http&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;headers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;cookie&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;regex&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;^(.*?;)?\s*(debug=1)\s*(;.*)?$&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;route&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;destination&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-service&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v2&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;route&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;destination&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-service&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The setup is still using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;match&lt;/code&gt; rules, but instead of depending on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uri&lt;/code&gt; matching, to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;headers&lt;/code&gt; for matching. Cookie is 1 of the special header in HTTP protocol so it could be used.&lt;/p&gt;

&lt;p&gt;A few more interesting rules can be found in &lt;a href=&quot;https://istio.io/latest/docs/reference/config/networking/virtual-service/#HTTPMatchRequest&quot;&gt;official documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;traffic-management---more-to-explore&quot;&gt;Traffic Management - more to explore&lt;/h2&gt;

&lt;p&gt;By reading until here, I hope you’ve realized that why Istio is a &lt;strong&gt;better&lt;/strong&gt; solution for traffic management - Its implementing L7 (Application) layer proxy is the key magic. Traffic management on the L4 (Transportation) layer have very limited flexibilities because it doesn’t have the higher layer’s information (Request, URL, Cookie, etc)&lt;/p&gt;

&lt;p&gt;In this artical, we’ve checked a few example of Istio doing traffice management around load balancing and routing, but there are more things we could tweak with Istio, like retries, outlier detection, etc. We will touch these points in following articles.&lt;/p&gt;</content><author><name>Yangyang Zhao</name></author><category term="istio" /><summary type="html">As many of the students who have studied computer science, networking is definitely a part that is very important, but easy to forget after graduation.</summary></entry><entry><title type="html">Istio Vs Envoy</title><link href="http://localhost:4000/istio/2023/11/20/Istio-Vs-Envoy.html" rel="alternate" type="text/html" title="Istio Vs Envoy" /><published>2023-11-20T17:00:00+08:00</published><updated>2023-11-20T17:00:00+08:00</updated><id>http://localhost:4000/istio/2023/11/20/Istio-Vs-Envoy</id><content type="html" xml:base="http://localhost:4000/istio/2023/11/20/Istio-Vs-Envoy.html">&lt;p&gt;If you looked at the documentation of Istio, this terminology &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;envoy&lt;/code&gt; appears multiple ones. Also, there are some concepts like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EnvoyFilter&lt;/code&gt;  that you might come up in the Istio documentation.&lt;/p&gt;

&lt;p&gt;Assuming you’ve already had experience of instealling Istio, you may find there are two essential kubernetes components - &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istiod&lt;/code&gt; deployment and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; sidecar container. And in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt; container you can find  there is this &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;envoy&lt;/code&gt; process forked from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pilot-agent&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
istio-p+     1  0.0  0.1 745700 36476 ?        Ssl  Oct19   2:29 /usr/local/bin/pilot-agent proxy sidecar --domain &amp;lt;app&amp;gt;.svc.cluster.local --serviceCluster &amp;lt;svc&amp;gt;.&amp;lt;ns&amp;gt; --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --log_out
istio-p+    33  8.0  0.2 496548 95728 ?        Sl   Oct19 376:12 /usr/local/bin/envoy -c etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --drain-strategy immediate --parent-shutdown-time-s 60 --service-cluster &amp;lt;svc&amp;gt;
istio-p+    78  0.0  0.0  18660  3328 pts/0    Ss   04:57   0:00 bash
istio-p+    95  0.0  0.0  34424  2712 pts/0    R+   04:58   0:00 ps aux
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The relationship between &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Istio&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Envoy&lt;/code&gt;, is like the relationship between &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Kubernetes&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Docker&lt;/code&gt;. &lt;a href=&quot;https://www.envoyproxy.io/&quot;&gt;Envoy&lt;/a&gt; is a standalone proxy tool designed for cloud native applications. If you take a look at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-proxy&lt;/code&gt;’s source code - https://github.com/istio/proxy, it is depending on https://github.com/envoyproxy/envoy.&lt;/p&gt;

&lt;p&gt;Configuring &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Istio&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Envoy&lt;/code&gt; is completely differnt, a typical Istio resource looks like&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;networking.istio.io/v1alpha3&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;VirtualService&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;reviews-route&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;hosts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;reviews.prod.svc.cluster.local&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;http&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;reviews-v2-routes&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;prefix&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/wpcatalog&quot;&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;prefix&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/consumercatalog&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;rewrite&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/newcatalog&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;route&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;destination&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;reviews.prod.svc.cluster.local&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v2&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;reviews-v1-route&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;route&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;destination&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;reviews.prod.svc.cluster.local&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;While &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;envoy&lt;/code&gt; configration looks like&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;static_resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;

  &lt;span class=&quot;na&quot;&gt;listeners&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;listener_0&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;address&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;socket_address&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;address&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;0.0.0.0&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;port_value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10000&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;filter_chains&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;filters&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;envoy.filters.network.http_connection_manager&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;typed_config&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;@type&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;stat_prefix&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ingress_http&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;access_log&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;envoy.access_loggers.stdout&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;typed_config&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;@type&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;http_filters&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;envoy.filters.http.router&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;typed_config&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;@type&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;type.googleapis.com/envoy.extensions.filters.http.router.v3.Router&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;route_config&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;local_route&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;virtual_hosts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;local_service&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;domains&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;*&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;routes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                  &lt;span class=&quot;na&quot;&gt;prefix&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/&quot;&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;route&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                  &lt;span class=&quot;na&quot;&gt;host_rewrite_literal&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;www.envoyproxy.io&lt;/span&gt;
                  &lt;span class=&quot;na&quot;&gt;cluster&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;service_envoyproxy_io&lt;/span&gt;

  &lt;span class=&quot;na&quot;&gt;clusters&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;service_envoyproxy_io&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For most of the Istio usages, you do not need to touch envoy related settings. Because 
&lt;a href=&quot;https://github.com/istio/istio/tree/master/pilot/cmd/pilot-agent&quot;&gt;pilot_agent&lt;/a&gt; does the job to &lt;strong&gt;translate your Istio configuration and other dynamic information into the envoy configurations&lt;/strong&gt;, and it bootstraps the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;envoy&lt;/code&gt; process, while &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;envoy&lt;/code&gt; actually does the job to handle all incoming / outgoing network traffic of the pod.&lt;/p&gt;

&lt;p&gt;Sometimes you still need to touch the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;envoy&lt;/code&gt; related configurations, if you use &lt;a href=&quot;https://istio.io/latest/docs/reference/config/networking/envoy-filter/&quot;&gt;Envoy Filters&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;networking.istio.io/v1alpha3&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;EnvoyFilter&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;custom-protocol&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;istio-config&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# as defined in meshConfig resource.&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;configPatches&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;applyTo&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;NETWORK_FILTER&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;SIDECAR_OUTBOUND&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# will match outbound listeners in all sidecars&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;listener&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;portNumber&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;9307&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;filterChain&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;envoy.filters.network.tcp_proxy&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;patch&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;operation&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;INSERT_BEFORE&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# This is the full filter config including the name and typed_config section.&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;envoy.extensions.filters.network.mongo_proxy&quot;&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;typed_config&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;@type&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;type.googleapis.com/envoy.extensions.filters.network.mongo_proxy.v3.MongoProxy&quot;&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;...&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;applyTo&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;NETWORK_FILTER&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# http connection manager is a filter in Envoy&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;c1&quot;&gt;# context omitted so that this applies to both sidecars and gateways&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;listener&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;filterChain&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;envoy.filters.network.http_connection_manager&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;patch&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;operation&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;MERGE&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;envoy.filters.network.http_connection_manager&quot;&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;typed_config&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;@type&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager&quot;&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;common_http_protocol_options&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;idle_timeout&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;30s&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Either modifying “pure” Istio or envoy configurations, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pilot-agent&lt;/code&gt; will translate the new configuraion into the envoy configuraion file, and cause &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;envoy&lt;/code&gt; to hot reload the new configurations.&lt;/p&gt;

&lt;p&gt;To get experience of modifying the spec part, we need to reference envoy’s own &lt;a href=&quot;https://www.envoyproxy.io/docs/envoy/latest/&quot;&gt;documentation site&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;From my personsal experience, envoy configuration is completely unfriendly to freshers of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Istio&lt;/code&gt;. The biggest problem of it is that it contains too many terminalogies, covers a lot of topics, but lacks of examples.&lt;/p&gt;

&lt;p&gt;Usually I would go to &lt;a href=&quot;https://github.com/envoyproxy/envoy/issues&quot;&gt;envoyproxy github issues&lt;/a&gt; and search for keywords. I could always get inspirations from the detailed configurations under the issue descriptions and replies.&lt;/p&gt;</content><author><name>Yangyang Zhao</name></author><category term="istio" /><summary type="html">If you looked at the documentation of Istio, this terminology envoy appears multiple ones. Also, there are some concepts like EnvoyFilter that you might come up in the Istio documentation.</summary></entry><entry><title type="html">Introduction</title><link href="http://localhost:4000/istio/2023/11/20/Introduction.html" rel="alternate" type="text/html" title="Introduction" /><published>2023-11-20T16:00:00+08:00</published><updated>2023-11-20T16:00:00+08:00</updated><id>http://localhost:4000/istio/2023/11/20/Introduction</id><content type="html" xml:base="http://localhost:4000/istio/2023/11/20/Introduction.html">&lt;p&gt;I started to get in touch with Istio from 2020, when the Istio version was still around 1.6-ish. I was not the person who introduced Istio to the company, there was another engineer who was also a newbie to Istio, spending some time studying it, then installed and created the initial setup of istio eco system, and the company started to use it.&lt;/p&gt;

&lt;p&gt;Unfortunately, not long after it was introduced, the guy who introduced Istio to the company resigned. He was the person who had the best knowledge of Istio by then (yet can not say he was a master of Istio). He tried his best to do the knowledge transfer before his last day, and I tried my best to digest them.&lt;/p&gt;

&lt;p&gt;Obviously I wasn’t able to absort that much, and I started my 3 years fighting with it. Unlike many other open source networking tools, like Haproxy for instance, the concept &amp;amp; usage of istio is not straightforward. The debugging of istio related issues is hard - sometimes you can’t just reproduce the issues 100%, but it happens, even with 0.x% of the possibility, but not acceptable if you want to achieve 99.9% or 99.99% availaibility.&lt;/p&gt;

&lt;p&gt;During 2020 ~ 2023, I met couple of issues that are related to istio, and I was able to spot and solve most the problems, but very ineffective. I spent days and days on verifying my hyposis, adjusting different settings, dug into the source code. Often the tryouts didn’t work, or even caused more problems. After solving the problems, the solutions seem to be very straightforward and I should have done it ever since adapting Istio.&lt;/p&gt;

&lt;p&gt;Every time I experience the “aha moment”, I would doubt that “Am I smart enought to use Istio? Was it a wrong decision to adapt Istio in the company given the learning curve is so steep?”.&lt;/p&gt;

&lt;p&gt;The fact is that to understand the Istio concepts, and master its usages, requires the person to have a good understanding of networking already. To most of the software engineers, it is hard, because networking was so well encapsulated so engineers barely need to write networking code in their daily life.&lt;/p&gt;

&lt;p&gt;This book is a series of articles that describe the understanding of istio related concepts, from a non vereran’s perspective.&lt;/p&gt;

&lt;p&gt;You don’t have to read this book if:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You do not use Istio&lt;/li&gt;
  &lt;li&gt;You are a veteran of Istio&lt;/li&gt;
  &lt;li&gt;You want to learn Istio in the “right” way&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yangyang Zhao</name></author><category term="istio" /><summary type="html">I started to get in touch with Istio from 2020, when the Istio version was still around 1.6-ish. I was not the person who introduced Istio to the company, there was another engineer who was also a newbie to Istio, spending some time studying it, then installed and created the initial setup of istio eco system, and the company started to use it.</summary></entry><entry><title type="html">How Do We Use Envoy To Improve Php Redis Client Performance</title><link href="http://localhost:4000/envoy/2021/07/30/How-do-we-use-Envoy-to-improve-PHP-Redis-client-performance.html" rel="alternate" type="text/html" title="How Do We Use Envoy To Improve Php Redis Client Performance" /><published>2021-07-30T16:00:00+08:00</published><updated>2021-07-30T16:00:00+08:00</updated><id>http://localhost:4000/envoy/2021/07/30/How-do-we-use-Envoy-to-improve-PHP-Redis-client-performance</id><content type="html" xml:base="http://localhost:4000/envoy/2021/07/30/How-do-we-use-Envoy-to-improve-PHP-Redis-client-performance.html">&lt;p&gt;Redis plays a very important role in Houzz’s technical stack - we used it for as  key-value storage, caching layer, queue system, etc.&lt;/p&gt;

&lt;p&gt;Redis uses standard TCP protocol for clients to connect to it, to connect to a single Redis instance, the client just need to establish a TCP connection with Redis server, and send raw command using text. You could easily test a connection to Redis server using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;telnet&lt;/code&gt; command.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash-3.2$ telnet 127.0.0.1 6379
Trying 127.0.0.1...
Connected to 127.0.0.1.
Escape character is &apos;^]&apos;.
PING
+PONG
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;redis-cluster&quot;&gt;Redis Cluster&lt;/h2&gt;

&lt;p&gt;As many other storage systems, Redis also has distributed mode -  it is called Redis cluster. The way how Redis scales from 1 instances to muiltiple instances is relying on the KEY hashing. As a key value storage system, Redis cluster will try to distributed all KEYs into different instances based on the CRC hash of the KEY. Each instance in the Redis cluster only stores KEYs within its own slots ranges, and all instances in the Redis custer will cover the whole range of CRC hash which is [0, 16384].&lt;/p&gt;

&lt;p&gt;Redis also support primary / secondary mode which could provides data redudancy and seperate read/update opertions. Each slot range can have 1 primary instance, with 1 or more replica instances storing the same KEYs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/imgs/redis_cluster.svg&quot; alt=&quot;Redis Cluster&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The operations on single Redis instance varies from it on Redis cluster. To use single Redis instance, we just connect to the single socket address, to use a Redis cluster, we would need to have more complicated logic. Typically, a Redis cluster client need to support the following features:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Automatically discover cluster topology. Since KEYs are stored in different Redis server instances, the Redis cluster client has to know which slot range is stored in which Redis server instances (including primary / replicas). Redis cluster provides a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CLUSTER SLOTS&lt;/code&gt; command that can return the server topology with arrays of slot range and primary/replica socket addresses. To execute the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CLUSTER SLOTS&lt;/code&gt; command, the Redis cluster client need to know at least the address of 1 nodes in the Redis cluster topology, which is usually passed in as start probe address(es) when inititalizing the client instance.&lt;/li&gt;
  &lt;li&gt;On demand dispatch commands to Redis server instance. When using Redis cluster client to send a command that carries KEY(s) parameter to the Redis cluster, the client has to determine which Redis server instance(s) should the command to sent to. How to detemine them is based on the server topology fetcher in previous step, to calculate the CRC hash of the KEY(s), then find the slot range that the KEY resides, and then based on the read / write policy, to send the command to the primary / replica Redis server instance of the slot range that the KEY belongs to. For some commands operating multiple KEYs (e.g. mget, mset, etc), the client has to split the single multiple key command into several batches based on the slot range group of the KEYs, and send the command with each group’s KEYs to the corresponding Redis server instance, and merge the results from all instances. Some client libraries didn’t implement this “advanced” feature, therefore they declare they don’t support multi-key commands with keys across slots.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There are already a lot of &lt;a href=&quot;https://redis.io/clients&quot;&gt;Redis client libraries&lt;/a&gt; developed in different languages that could help to mitigate the difference of connecting to a single Redis client or Redis cluster.  In most of the senarios, programmers won’t notice the difference of using single Redis instance or Redis cluster, since the complexity is capsulated by the client libraries.&lt;/p&gt;

&lt;h2 id=&quot;php-redis-cluster-client-and-the-problem&quot;&gt;PHP Redis Cluster Client and the Problem&lt;/h2&gt;

&lt;p&gt;A single Redis instance can satisfy the small use cases, at the scale of supporting Houzz’s business, we have to Redis cluster vs single Redis instance since we’ve stored TBs data in Redis and it has to be distributed into different instances.&lt;/p&gt;

&lt;p&gt;As the time of 2021, we’ve setup a Redis cluster with 200+ instances, with primary / replica enabled, to make sure we have data redundancy and ability to recover from single instance failure.&lt;/p&gt;

&lt;p&gt;As historical reasons, Houzz’s major programming language is PHP, it is still one of the most popular programming language in web development. And we use &lt;a href=&quot;https://github.com/predis/predis&quot;&gt;predis&lt;/a&gt;, a widely used PHP Redis client that supports Redis cluster, it provides the feature such as automatic cluster topology discovery and on demand dispatch commands to Redis server instance.&lt;/p&gt;

&lt;p&gt;Funtionality wise, we are satisified with the PHP redis client, however, the nature of PHP language itself gives us very big challange on the performance side.&lt;/p&gt;

&lt;p&gt;Unlike Java, Python or Node server applications, PHP process does not share data between subsequent requests. Once PHP handles a web request, it will discard all the data before it’s ready to handle the next request.&lt;/p&gt;

&lt;p&gt;Why does this hurt the performance of the Redis cluster client?&lt;/p&gt;

&lt;p&gt;First of all, the cluster topology will not be shared across requests. Suppose predis first connect to one of the start probe redis server instance, send &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CLUSTER SLOTS&lt;/code&gt; command, and got the cluster topology in the handling of one request, the topology, which is usually a nested &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;array&lt;/code&gt; in memory will not be used in the next request. Therefore, in the next request, predis has to connect to one of the start probe again, and send &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CLUSTER SLOTS&lt;/code&gt; again to get the cluster topology. This is exremely ineffective since Redis cluster topology rarely changes (usually it only changes when we take some redis nodes down or add new redis nodes, which will cause a rebalance). The extra &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CLUSTER SLOTS&lt;/code&gt; command usually add up 4 ~ 5 ms to the redis access time per request, if in the request, there are only 1 redis operation needed, then we added 100% overhead than it’s supposed to be.&lt;/p&gt;

&lt;p&gt;Another nature behavior of PHP process is that, you can not persist any TCP connections between two requests.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/imgs/redis_connection.svg&quot; alt=&quot;Redis Connection&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As described in the above graph, suppose we have 2 PHP processes running on a single machine, with 2 requests been handled per process, and each request will need to connect to 2 Redis server instances. The subsequantial requests handled by the same process will connect to the same redis server instance, yet since the previous request’s TCP connection is closed, it can not be reused, so even it connects to the same socket address (of the Redis server instance), it has to create another TCP connection.&lt;/p&gt;

&lt;p&gt;So as a PHP process is handling more and more requests, it creates / closes more and more TCP connections to Redis clusters. Compared to other languages which could maintain persistent TCP connections in the process, this is already a big performance loss.&lt;/p&gt;

&lt;p&gt;Another more critical problem is, a TCP connection’s termination is not a direct close and not releasing the resource immediately.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/imgs/tcp.svg&quot; alt=&quot;TCP lifecycle&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As described in the above graph, in active close mode, when the TCP connection initiator (in this case the PHP redis cluster client) started to close the the connection, it sends a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FIN&lt;/code&gt; packet through the TCP connection, the TCP connectino receiver (Redis cluster instance) will need to send &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ACK&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FIN&lt;/code&gt; packet back. Due to the nature of TCP connection, initiator will turn into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TIME_WAIT&lt;/code&gt; status until finally it timeout and connection is finally closed, the TCP connection resource is recycled by system.&lt;/p&gt;

&lt;p&gt;Due to the PHP Redis cluster client access pattern, it keeps creating new TCP connections to PHP Redis cluster instances and closes them immediately after 1 session ends. Due to the QPS of our production traffic volume to 1 box, it leaves too many  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TIME_WAIT&lt;/code&gt; TCP connections in the box. Once &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TIME_WAIT&lt;/code&gt; TCP connections pile up, new TCP connections are slower to be created, thus making Redis cluster client to be slower, and causing web request responding time to increase, more requests are stucking. In the end, it reaches the threshold that Redis cluster client failed to create TCP connections within the timeout settings time (4s), and it will throw an error saying Redis server is down.&lt;/p&gt;

&lt;p&gt;We noticed this issue from last year when our access volume significant increased (from business perspective it’s a very good sign!), and we were seeing more errors complaining about Redis server down and performance degradation during the peek hours.  But Redis servers actually are not on high pressure at all, the bottleneck is purely from the TCP connections from the client side. While to solve this, every time SRE team has to increase the instances of our application boxes, which is unnessasary, and a waste of our computing resources.&lt;/p&gt;

&lt;h2 id=&quot;finding-solutions&quot;&gt;Finding Solutions&lt;/h2&gt;

&lt;p&gt;If you google/stackoverflow “too many TIME_WAIT tcp connections” keywords, some of the blogs will lead to the following solution: Enable tcp resuse feature to reduce the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TIME_WAIT&lt;/code&gt; TCP connections.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sysctl -w net.ipv4.tcp_tw_reuse=1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;We believed this would work, and added this option to the system default setting of our production boxes, however, it didn’t help. The TCP connections in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TIME_WAIT&lt;/code&gt; state are still high.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@web-server:/# sysctl net.ipv4.tcp_tw_reuse=1
net.ipv4.tcp_tw_reuse = 1
root@web-server:/# ss -tan state time-wait | grep 6379 | wc -l
70145
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Inspired by &lt;a href=&quot;https://vincent.bernat.ch/en/blog/2014-tcp-time-wait-state-linux#netipv4tcp_tw_reuse&quot;&gt;Coping with the TCP TIME-WAIT state on busy Linux servers&lt;/a&gt; blog post, we think we found the reason why it didn’t work.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;By enabling net.ipv4.tcp_tw_reuse, Linux will reuse an existing connection in the TIME-WAIT state for a new outgoing connection if the new timestamp is strictly bigger than the most recent timestamp recorded for the previous connection: an outgoing connection in the TIME-WAIT state can be reused after just one second.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Within “Just one second”, since the QPS of the box is around 100, it will probably create ~ 100 (avg QPS) * 50 (avg # of redis connections in 1 request) = 5,000 new TCP connections, and these 5K connections can not reuse the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TIME_WAIT&lt;/code&gt; connections left intact in the exact second. Before it could benefit from the tcp reuse mechanism, the too many &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TIME_WAIT&lt;/code&gt; TCP connections hold up the kernal from creating new TCP connections, thus this didn’t help much.&lt;/p&gt;

&lt;h2 id=&quot;envoy--envoy-redis-cluster&quot;&gt;Envoy / Envoy Redis Cluster&lt;/h2&gt;

&lt;p&gt;Since the year of 2020, we introduced Istio as our service mesh solution to our Kubernetes cluster, and it started to play as the proxy for all network traffic from / to our web application container.&lt;/p&gt;

&lt;p&gt;Envoy, as the proxy implementation of Istio, provides a lot of features that helps the abstract the networking logic from application code.&lt;/p&gt;

&lt;p&gt;One inspiring feature that Envoy provides in the newer versions, which is the final solution we used for solving this issue, is &lt;a href=&quot;https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/other_protocols/redis#redis-cluster-support-experimental&quot;&gt;Redis Cluster support&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;When using Envoy as a sidecar proxy for a Redis Cluster, the service can use a non-cluster Redis client implemented in any language to connect to the proxy as if it’s a single node Redis instance. The Envoy proxy will keep track of the cluster topology and send commands to the correct Redis node in the cluster according to the spec. Advance features such as reading from replicas can also be added to the Envoy proxy instead of updating redis clients in each language.&lt;/p&gt;

  &lt;p&gt;Envoy proxy tracks the topology of the cluster by sending periodic cluster slots commands to a random node in the cluster, and maintains the following information:&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;List of known nodes.&lt;/li&gt;
    &lt;li&gt;The primaries for each shard.&lt;/li&gt;
    &lt;li&gt;Nodes entering or leaving the cluster.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;With this feature enabled, mthe PHP Redis client’s cluster related logic will be taken over by Envoy. PHP Redis client only need to create one connection to the proxied Redis instance provided by Envoy.&lt;/p&gt;

&lt;p&gt;It took us for a while to figure out how exactly to configure the Envoy Redis cluster proxy, due to the lack of examples provided. But we finally figured out the configuration, first of all, we need to create a backend cluster.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;networking.istio.io/v1alpha3&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;EnvoyFilter&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;redis-cluster-backend&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;configPatches&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;applyTo&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;CLUSTER&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;patch&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;operation&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;INSERT_FIRST&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;cluster_type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;envoy.clusters.redis&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;typed_config&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;@type&apos;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;type.googleapis.com/google.protobuf.Struct&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;cluster_refresh_rate&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;1800s&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;cluster_refresh_timeout&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;4s&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;host_degraded_refresh_threshold&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;redirect_refresh_interval&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;4s&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;redirect_refresh_threshold&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;connect_timeout&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;4s&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;dns_lookup_family&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;V4_ONLY&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;load_assignment&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;cluster_name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;custom-redis-cluster&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;endpoints&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;lb_endpoints&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;endpoint&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;address&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                  &lt;span class=&quot;na&quot;&gt;socket_address&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;address&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;redis instance 1 ip&amp;gt;&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;port_value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;redis instance 1 port&amp;gt;&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;endpoint&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;address&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                  &lt;span class=&quot;na&quot;&gt;socket_address&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;address&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;redis instance 2 ip&amp;gt;&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;port_value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;redis instance 1 port&amp;gt;&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;endpoint&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;address&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                  &lt;span class=&quot;na&quot;&gt;socket_address&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;address&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;redis instance 3 port&amp;gt;&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;port_value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;redis instance 3 port&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;redis-cluster-backend&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;outlier_detection&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;base_ejection_time&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;20s&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;consecutive_5xx&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;interval&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;5s&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;upstream_connection_options&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;tcp_keepalive&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;keepalive_interval&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;keepalive_probes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;keepalive_time&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;30&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the backend settings, the most important settings is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;load_assignment&lt;/code&gt;, it basically describes the startuo probe nodes that envoy will send &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CLUSTER SLOTS&lt;/code&gt; command to get the cluster topology.&lt;/p&gt;

&lt;p&gt;There are definitely quite a few settings worth being tuned to get better performance:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;cluster_refresh_rate&lt;/strong&gt;:  This is the interval between &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CLUSTER SLOTS&lt;/code&gt; commands sent from envoy to startup probe nodes. Typically redis cluster topology won’t change that frequently, so we could set this value much longer than default value (5s)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;host_degraded_refresh_threshold&lt;/strong&gt;: The number of hosts became degraded or unhealthy before triggering a topology refresh request. This is very helpful, since we will configure &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cluster_refresh_rate&lt;/code&gt; to be as long as possible, once the redis cluster topology changed between two refresh, this config setting to a low value (1 for most aggresive) will immediately let Envoy to send &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CLUSTER SLOS&lt;/code&gt; command to get the new topology, to remove potential problematic node out of the connection pools.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;redirect_refresh_threshold&lt;/strong&gt;: The number of redirection errors that must be received before triggering a topology refresh request. Sometimes, when we add / remove nodes from Redis cluster, the key will rebalance among the new cluster instances, it will cause some response to be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MOVED&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ASK&lt;/code&gt;, in this case, we should let Envoy to send &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CLUSTER SLOS&lt;/code&gt; command to get the new topology immeidately.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;outlier_detection&lt;/strong&gt;: &lt;a href=&quot;https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/upstream/outlier&quot;&gt;Outlie detection&lt;/a&gt; is unified mechanism in Envoy to detect outlited upstream hosts. In Envoy Redis cluster, an upstream means 1 redis cluster server in the topology. Suppose a redis cluster node was suddenly down, the following request to the node will get ERR response. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;consecutive_5xx&lt;/code&gt; setting is the threshold of ERR responses in past &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;interval&lt;/code&gt; time so as to consider the node to be unhealthy. Once the node is considered unhealthy, it will be taken out of the upstream servers, and will tried to be put back after &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;base_ejection_time&lt;/code&gt;, and exponentially backoff in following degradations.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;upstream_connection_options&lt;/strong&gt;: This is mostly for tuning the keep alive setting for TCP connections between Envoy and Redis cluster nodes. Envoy will on demandly connect to Redis cluster nodes if PHP client tries to send a command to the destination nodes, after the command is sent, the connection remains &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ESTABLISED&lt;/code&gt; state. But if the connnection stays too long time without sending any packages to Redis cluster node, it might be closed by the Redis cluster node, in order to extend the life of the TCP connection, we could configure Envoy sending keepalive packet to Redis cluster node after &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;keepalive_time&lt;/code&gt; since the connection is established, and set it to send &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;keepalive_probes&lt;/code&gt; times keepalive interval with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;keep_alive&lt;/code&gt; interval.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We also need to create a frontend in Envoy to let the PHP client in main container to connect.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;networking.istio.io/v1alpha3&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;EnvoyFilter&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;redis-cluster-frontend&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;configPatches&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;applyTo&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;LISTENER&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;SIDECAR_OUTBOUND&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;patch&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;operation&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ADD&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;address&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;socket_address&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;address&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;127.0.10.1&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;port_value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;6379&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;TCP&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;filter_chains&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;filters&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;envoy.filters.network.redis_proxy&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;typed_config&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;@type&apos;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;type.googleapis.com/envoy.extensions.filters.network.redis_proxy.v3.RedisProxy&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;prefix_routes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;catch_all_route&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                  &lt;span class=&quot;na&quot;&gt;cluster&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;redis-cluster-backend&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;settings&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;enable_hashtagging&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;enable_redirection&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;op_timeout&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;4s&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;read_policy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;PREFER_REPLICA&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;stat_prefix&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;redis_proxy&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;redis-cluster-frontend&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above configuraion creates a Redis proxy frontend on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;127.0.10.1:6379&lt;/code&gt;. This address is available to the other containers aside from istio-proxy. The most importan config, is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefix_routes.catch_all_route&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;redis-cluster-backend&lt;/code&gt; cluster which is the exact backend cluster we created before. The config of proxy frontend is relatively simple, with fewer settings could be tuned, one important setting is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;read_policy&lt;/code&gt;, we chose &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PREFER_REPLICA&lt;/code&gt; to achive the functional parity as the original PHP Redis cluster client’s logic - when reading from redis (get, mget, etc), always try to read from replica nodes, only read from primary nodes when all replica nodes for the slots were not available.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/imgs/istio.svg&quot; alt=&quot;Envoy Proxy Cluster&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The above graph directly shows the magic why using Envoy Redis cluster proxy could save TCP connections.&lt;/p&gt;

&lt;p&gt;Before, suppose in 1 request handling process PHP Redis client needs to connect to X Redis cluster instaces, it has to create X TCP connections from main web server container to istio-proxy container, and envoy has to create corresponding X TCP connections from istio-proxy container to Redis cluster instances.&lt;/p&gt;

&lt;p&gt;After using the Envoy Redis cluster proxy, PHP client only needs to connect to the frontend proxy in istio-proxy container, and Envoy will “smartly” dispatch the commands to proxy backend, and create connections to X Redis cluster servers.&lt;/p&gt;

&lt;p&gt;We saved X - 1 TCP connections for one request handled by the PHP process. What’s more, the X connections created between Envoy Redis cluster proxy backend and the real Redis cluster services are persistent connections, they do not close after the PHP process finished handling 1 request. Also, other requests handled by other PHP processes in the main web server container could also reuse the connections since the frontend and backend is seperated. These all leads to a huge save of TCP connections.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@web-server:/# ss -tan state time-wait | grep 6379 | wc -l
3108
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;By doing this, we broke the nature limitation of PHP language, we reduced &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TIME_WAIT&lt;/code&gt; tcp connections by 95%!&lt;/p&gt;

&lt;p&gt;Also, we see great reduce of average redis connection time per request dropped from ~5ms to nearly 0ms! This is contributed by:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Much fewer connections needs to be made since all commands are sent to to the same frontend proxy by PHP client, thus only 1 connection needs to be created per requet.&lt;/li&gt;
  &lt;li&gt;Connection to the frontend is via local network (from main container to istio-proxy container), which is much stable and lightweighted.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We roll out this solution 100% on our production k8s clusters, and indeed, it solved the bottleneck of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TIME_WAIT&lt;/code&gt; TCP connections, and enabled our Pods to be able to take more traffic. It also leads to some save of the k8s resources.&lt;/p&gt;

&lt;h2 id=&quot;caveats&quot;&gt;Caveats&lt;/h2&gt;

&lt;p&gt;While we talked all good things about Envoy Redis cluster proxy, there are certainly some caveats for using it.&lt;/p&gt;

&lt;h3 id=&quot;read--write-performance-degradation&quot;&gt;Read / Write performance degradation&lt;/h3&gt;
&lt;p&gt;We mentioned the average TCP connection time reduced from ~ 5m to nearly 0ms in the last chapter. But for read / write redis operations, the performance degradated a bit. This is easy to understand. Envoy still needs to create the TCP connections to Redis cluster nodes, and it still needs to send the command packages to the nodes, and receive the responses, even more logic was added to dispatch commands to the correct node(s) and aggregate responses from multiple nodes (e.g. mget, mset). This degradation was more obvious for sparse request when the TCP connection reuse in the proxy backend wasn’t giving much benefit.&lt;/p&gt;

&lt;p&gt;We noticed this consistent performance degradation while we were finally OK with it - the overall average read / write time was fast enough comparing to rest of PHP code logic.&lt;/p&gt;

&lt;h3 id=&quot;lack-of-retry-logic&quot;&gt;Lack of retry logic&lt;/h3&gt;

&lt;p&gt;There was an very important feature in the old PHP Redis Cluster client, which is the auto retry logic.&lt;/p&gt;

&lt;p&gt;Generally for each shard of the Redis cluster, it consists of 1 primary node and 2 replica nodes. When PHP Redis cluster client is to send a read command, it first will pick a random replica to send the command, if the command failed due to network commnunication issue, it will retry to send the command to the other replica node, if it still fails, it will ultimately send the command to the primary node.&lt;/p&gt;

&lt;p&gt;This logic is straightforward and is very helpful to mitigate the downtime. Suppose we are doing a rolling restart of the whole Redis cluster, typically we will restart 1 node at one time, the retry logic will make sure the request can still be properly handled.&lt;/p&gt;

&lt;p&gt;Unfortunately, Built-in retry logic is still on the &lt;a href=&quot;https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/other_protocols/redis&quot;&gt;planned future features list&lt;/a&gt; at the moment of Mid 2021, as we tested putting down 1 replica node in 1 shard, we noticed there were around 10 seconds downtime when we see client errors of “no upstream hosts” from Envoy. This down time could be mitigated by tuning the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;outlier_detection&lt;/code&gt; settings, when the node is down, subsequential requests to the specific node will be errored out, after the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;outlier_detection.interval&lt;/code&gt; time, it will be taken out of the upstreams and Envoy will stop sending any more commands to this node.&lt;/p&gt;

&lt;p&gt;Currently there is no good way to bypass this issue. Even we retry within the PHP client, the PHP client couldn’t decide which Redis cluster node that Envoy will connect.&lt;/p&gt;

&lt;p&gt;We lived with this lack of retry logic, and rely on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;outlier_detection&lt;/code&gt; mechanism to mitigate the downtime, and we really hope the built-in retry logic could be supported soon.&lt;/p&gt;

&lt;h3 id=&quot;too-many-established-connections&quot;&gt;Too many ESTABLISHED connections&lt;/h3&gt;

&lt;p&gt;This is a problem we forsee, that before our PHP clients create TCP connections to Redis cluster nodes, and disconnect soon after the request is processed. The “come and go” pattern actually has a benefit that overall Redis cluster nodes do not have too many &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ESTABLISHED&lt;/code&gt; connections from the clients.&lt;/p&gt;

&lt;p&gt;Ever since we start testing using Envoy Redis cluster proxy, we immediately noticed dramatic increase of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ESTABLISHED&lt;/code&gt; connections from the Redis server nodes. The increased persistent connections have been impacted the connection performance - if 1 Redis cluster node has been connected by too many clients, newer connections will be hard to made.&lt;/p&gt;

&lt;p&gt;To improve this, we tuned down &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;timeout&lt;/code&gt; setting in redis.conf on the Redis server side from 300 seconds to half of the value, and it helped to reduce 30% of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ESTABLISHED&lt;/code&gt; connections in our workload. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;timeout&lt;/code&gt; setting let Redis server side to close the connection after a client is idle for that long time. However, there is always the trade off - The shorter we configured the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;timeout&lt;/code&gt; setting, the more likely Envoy will have to connect more frequently to Redis cluster nodes, and impact the overall Redis client performance.&lt;/p&gt;

&lt;p&gt;Another solution, is to increase the Redis cluster nodes number, so average connections to 1 Redis cluster nodes will drop, however, this will cause unessasary resource waste if Redis workload is not high on each node.&lt;/p&gt;</content><author><name>Yangyang Zhao</name></author><category term="envoy" /><summary type="html">Redis plays a very important role in Houzz’s technical stack - we used it for as key-value storage, caching layer, queue system, etc.</summary></entry></feed>